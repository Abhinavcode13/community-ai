{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "nTBaLnHVfBxd"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "import faiss"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def read_files_in_repo(repo_path, file_extensions=None):\n",
        "    \"\"\"\n",
        "    Recursively read all files in the repo with the given file extensions.\n",
        "    Returns a list of (filename, file_text).\n",
        "    \"\"\"\n",
        "    if file_extensions is None:\n",
        "        file_extensions = [\".md\", \".py\", \".js\", \".ts\", \".txt\", \".html\", \".css\",\"ipynb\"]\n",
        "\n",
        "    filepaths = []\n",
        "    for ext in file_extensions:\n",
        "        filepaths.extend(glob.glob(os.path.join(repo_path, f\"**/*{ext}\"), recursive=True))\n",
        "\n",
        "    file_contents = []\n",
        "    for fp in filepaths:\n",
        "        try:\n",
        "            with open(fp, \"r\", encoding=\"utf-8\") as f:\n",
        "                text = f.read()\n",
        "                if text.strip():\n",
        "                    file_contents.append((fp, text))\n",
        "        except Exception as e:\n",
        "            print(f\"Could not read file {fp}: {e}\")\n",
        "    return file_contents\n",
        "\n",
        "def chunk_text(text, chunk_size=500, overlap=50):\n",
        "    \"\"\"\n",
        "    Split text into overlapping chunks of chunk_size characters (naive approach).\n",
        "    \"\"\"\n",
        "    chunks = []\n",
        "    start = 0\n",
        "    while start < len(text):\n",
        "        end = min(start + chunk_size, len(text))\n",
        "        chunk = text[start:end]\n",
        "        chunks.append(chunk)\n",
        "        start += (chunk_size - overlap)\n",
        "    return chunks\n",
        "\n",
        "repo_path = \"/content/community-ai\"  # Change to your local path if needed\n",
        "\n",
        "file_contents = read_files_in_repo(repo_path)\n",
        "chunks_data = []\n",
        "for (filename, text) in file_contents:\n",
        "    for chunk in chunk_text(text, chunk_size=700, overlap=200):\n",
        "        chunks_data.append({\"filename\": filename, \"content\": chunk})\n",
        "\n",
        "print(\"Number of chunks:\", len(chunks_data))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xgFKBWpTfNlt",
        "outputId": "1ad64843-be1f-441e-effa-e2f8598bf667"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of chunks: 794\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "embedding_model = SentenceTransformer(embedding_model_name)\n",
        "\n",
        "# Create embeddings for each chunk\n",
        "chunk_embeddings = []\n",
        "for doc in chunks_data:\n",
        "    embedding = embedding_model.encode(doc[\"content\"])\n",
        "    chunk_embeddings.append(embedding)\n",
        "\n",
        "chunk_embeddings = np.array(chunk_embeddings)\n",
        "print(chunk_embeddings.shape)  # Should be (#chunks, embedding_dim)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NXROmbZ9fYIy",
        "outputId": "02a56dff-07fc-469b-815c-1a84db147754"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(794, 384)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install faiss-cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MHZCcfKvka3r",
        "outputId": "e8210f39-1eed-4e71-aa0f-51799a6c2592"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "Downloading faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl (30.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/30.7 MB\u001b[0m \u001b[31m63.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "d = chunk_embeddings.shape[1]  # Dimensionality of embeddings\n",
        "index = faiss.IndexFlatL2(d)   # L2 distance index\n",
        "index.add(chunk_embeddings)\n",
        "\n",
        "print(f\"Total embeddings indexed: {index.ntotal}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XPFTap6TkV1o",
        "outputId": "7fb83c5b-4bfd-4dd9-86da-5558d9934e73"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total embeddings indexed: 794\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve_chunks(query, k=5):\n",
        "    # Embed the query\n",
        "    query_embedding = embedding_model.encode(query)\n",
        "    query_embedding = np.array([query_embedding], dtype=np.float32)\n",
        "\n",
        "    # Search\n",
        "    distances, indices = index.search(query_embedding, k)\n",
        "\n",
        "    # Grab the corresponding chunks\n",
        "    retrieved = []\n",
        "    for dist, idx in zip(distances[0], indices[0]):\n",
        "        retrieved.append((dist, chunks_data[idx]))\n",
        "    return retrieved\n"
      ],
      "metadata": {
        "id": "DI7HayhFldQu"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from groq import Groq\n",
        "\n",
        "# Instantiate a Groq client using your API key.\n",
        "client = Groq(api_key=\"add your token\")\n",
        "\n",
        "def generate_answer(query, retrieved_docs):\n",
        "    \"\"\"\n",
        "    Builds a prompt that includes the retrieved repository context and the user question.\n",
        "    Then it sends this prompt to the Groq Chat API and returns the assistant’s answer.\n",
        "    \"\"\"\n",
        "    # Combine the retrieved document chunks into a context string.\n",
        "    context = \"\"\n",
        "    for _, doc in retrieved_docs:\n",
        "        context += f\"\\n[File: {doc['filename']}]\\n{doc['content']}\\n\"\n",
        "\n",
        "    # Define the conversation for Groq.\n",
        "    system_prompt = (\"You are a helpful assistant. Use the following repository context from its repo contents \"\n",
        "                     \"to answer the user's question.\")\n",
        "    user_prompt = f\"Context: {context}\\nQuestion: {query}\\nAnswer:\"\n",
        "\n",
        "    # Create a chat completion request using Groq.\n",
        "    chat_completion = client.chat.completions.create(\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\",   \"content\": user_prompt}\n",
        "        ],\n",
        "        model=\"qwen-qwq-32b\",  # Select a suitable model from Groq's offerings.\n",
        "        # max_completion_tokens=max_tokens, use it as per need, remove it in case of reasoning model for better response\n",
        "        temperature=0.5,    # Adjust temperature as needed.\n",
        "        top_p=0.9,        # Adjust top_p as needed.\n",
        "        stream=False\n",
        "    )\n",
        "\n",
        "    # Extract and return the answer (trimming whitespace).\n",
        "    output = chat_completion.choices[0].message.content\n",
        "    return output.strip()\n",
        "\n",
        "def chat_with_github_repo(query, k=5):\n",
        "    # Retrieve top-k document chunks (assumes retrieve_chunks is defined elsewhere).\n",
        "    retrieved = retrieve_chunks(query, k=k)\n",
        "    # Generate an answer using Groq's Chat API.\n",
        "    answer = generate_answer(query, retrieved)\n",
        "    return answer\n",
        "\n",
        "# Example usage:\n",
        "user_query = \"can you explain the purpose of this whole repo? and what does it do.\"\n",
        "answer = chat_with_github_repo(user_query, k=5)\n",
        "reasoning_output= answer.split(\"</think>\")[0]\n",
        "print(reasoning_output)\n",
        "answer_from_model=answer.split(\"</think>\")[-1]\n",
        "print(\"Answer to Query: \",answer_from_model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "grJkYz8Elg1B",
        "outputId": "6fddcd87-5ec7-44e6-b066-6a479a641d7e"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<think>\n",
            "Okay, let's try to figure out what the user is asking here. They want to know the purpose of the entire repo and what it does. The files mentioned are all under the path /content/community-ai/Data Scraping/DataExtraction.ipynb. The content snippets provided seem to be from a Confluence page or a JIRA-related document, given mentions of Atlassian, JIRA, and Confluence macros. \n",
            "\n",
            "Looking at the content, it's talking about a \"Projects\" space where they track major work on a product, specifically mentioning Mifos Forge and OpenMRS as examples. The space is meant for contributors, financial institutions, and new volunteers to collaborate, list feature needs, and find projects. The JIRA references suggest that actual tasks are tracked there as epics and user stories, while this space serves as a central hub linking to those tasks and related documents.\n",
            "\n",
            "The user might be confused because the repo's name includes \"community-ai\" and \"Data Scraping,\" but the files' content is about project management. Maybe the repo is part of a larger system where data scraping is used to extract project information from Confluence or JIRA. Alternatively, the DataExtraction.ipynb could be a script meant to scrape data from these project tracking pages for analysis or automation.\n",
            "\n",
            "The mention of cookies and tracking notices, along with HTML elements like avatars and preferences, suggests the content is from a web page. The repo might be related to maintaining or automating the management of project documentation. The purpose could be to organize and streamline how projects are tracked and collaborated on within the community, using data extraction to keep everything up-to-date or to generate reports.\n",
            "\n",
            "I should check if the repo's structure or other files (not shown here) have more context, but based on the given files, the main purpose seems to be facilitating collaboration on projects through centralized documentation linked to JIRA tasks. The data extraction notebook might automate parts of this process, like pulling data from Confluence pages or JIRA to aggregate project info. The answer should clarify that the repo supports project management by providing a central space and using data scraping to handle information efficiently.\n",
            "\n",
            "Answer to Query:  \n",
            "\n",
            "The repository you're referring to appears to be part of a collaborative project management system, likely related to the **Mifos Initiative** (a financial services platform for microfinance institutions) or a similar open-source project. Here's an explanation of its purpose and functionality based on the provided context:\n",
            "\n",
            "---\n",
            "\n",
            "### **Purpose of the Repo**\n",
            "The repository is designed to **centralize and organize project management and collaboration** for community-driven development. Specifically:\n",
            "1. **Project Tracking**: \n",
            "   - It serves as a hub for tracking major projects or features being developed for a product (e.g., the Mifos platform).\n",
            "   - Projects are managed using **JIRA** (an Atlassian tool), where work is broken into **epics** and **user stories** for task tracking.\n",
            "   - The repo links to these JIRA tasks, design documents, specifications, and background materials, creating a single source of truth for contributors.\n",
            "\n",
            "2. **Community Collaboration**:\n",
            "   - **Financial institutions** can list feature requests or project needs they require help with.\n",
            "   - **New contributors** can find projects to join, fostering community involvement.\n",
            "   - **Current contributors** can collaborate on active projects by accessing documentation and task updates.\n",
            "\n",
            "3. **Documentation and Design**:\n",
            "   - It hosts background information, design documents, and specifications related to each project.\n",
            "   - The structure is inspired by frameworks like **OpenMRS** (another open-source healthcare project), which uses a project-based approach to organize work.\n",
            "\n",
            "---\n",
            "\n",
            "### **What the Repo Does**\n",
            "- **Centralizes Project Information**: \n",
            "  - Aggregates all resources (JIRA tickets, design docs, specifications) for active projects in one place.\n",
            "  - The `DataExtraction.ipynb` notebook (a Jupyter notebook) might be used to **scrape or process data** from platforms like Confluence or JIRA to automate documentation or track project progress.\n",
            "  \n",
            "- **Facilitates Collaboration**:\n",
            "  - Provides a space for stakeholders (developers, institutions, volunteers) to communicate and coordinate work.\n",
            "  - Uses Atlassian tools (Confluence pages, JIRA) for task management and version control.\n",
            "\n",
            "- **Supports Open-Source Practices**:\n",
            "  - Encourages transparency by documenting project goals, requirements, and progress.\n",
            "  - Helps new contributors understand how to get involved by highlighting available projects and tasks.\n",
            "\n",
            "---\n",
            "\n",
            "### **Key Components Mentioned in the Files**\n",
            "- **`DataExtraction.ipynb`**: \n",
            "  - Likely a Python notebook for **data scraping or extraction** from platforms like Confluence or JIRA. This could automate tasks like:\n",
            "    - Pulling project details (e.g., JIRA epics/user stories).\n",
            "    - Generating reports or summaries of project statuses.\n",
            "    - Cleaning or structuring data from unstructured documentation.\n",
            "\n",
            "- **Confluence/JIRA Integration**:\n",
            "  - The repo links to **Atlassian Confluence pages** (e.g., the \"Projects\" space) and JIRA for task tracking. The snippets include references to Atlassian's cookies, UI elements, and macros, suggesting the repo is part of a broader documentation or automation system for these tools.\n",
            "\n",
            "- **Community-Oriented Workflow**:\n",
            "  - Aims to streamline how contributors, institutions, and volunteers collaborate on projects, ensuring all artifacts (designs, specs, code) are linked and accessible.\n",
            "\n",
            "---\n",
            "\n",
            "### **Why It Matters**\n",
            "This repo is critical for:\n",
            "- Keeping the community aligned on project goals and progress.\n",
            "- Reducing fragmentation by centralizing information.\n",
            "- Enabling automation (via scripts like `DataExtraction.ipynb`) to manage and analyze project data efficiently.\n",
            "\n",
            "If you need more specifics, check the `DataExtraction.ipynb` code to see exactly how data is scraped/processed, or look for README files in the repo for explicit project descriptions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_query = \"what does the android-client do in context of this repo?\"\n",
        "answer = chat_with_github_repo(user_query, k=5)\n",
        "answer=answer.split(\"</think>\")[-1]\n",
        "print(answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zFU-qp5anHTc",
        "outputId": "e5309494-fd67-424e-98a0-93b90d6952e0"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "The **Android Client** in the context of this repository refers to a **chatbot tool** designed to assist with queries related to the **Android-Client project** of the Mifos system. Here's a breakdown of its role and context:\n",
            "\n",
            "### Key Details:\n",
            "1. **Purpose**:\n",
            "   - The Android Client component in this repository is a **Q&A chatbot** (built using Gradio) that allows users to ask questions about the **Kotlin code** of the Mifos Android app.\n",
            "   - It enables developers or users to query the codebase of the Android app (e.g., \"How does a specific feature work?\") and receive answers based on the code's documentation, comments, or structure.\n",
            "\n",
            "2. **Technical Implementation**:\n",
            "   - The chatbot is defined in the file [`android-client_bot.ipynb`](/content/community-ai/Android-Client/android-client_bot.ipynb), where it uses a `gr.Interface` (Gradio) to create a web-based interface.\n",
            "   - It processes questions via the `answer_question_with_parent_docs` function, which likely leverages vector embeddings or semantic search to retrieve relevant code-related answers.\n",
            "\n",
            "3. **Historical Context**:\n",
            "   - The original **Android Client Project** (from 2015, linked in Jira) was part of a summer internship to develop an Android app for Mifos, a financial inclusion platform. This historical project forms the codebase the chatbot analyzes.\n",
            "   - The current repository's Android Client tool focuses on **knowledge extraction** from that legacy code, helping new contributors or maintainers understand it more easily.\n",
            "\n",
            "4. **Repository Structure**:\n",
            "   - The tool is part of the `community-ai` project, which aims to build AI-driven tools for Mifos (e.g., chatbots, code analysis).\n",
            "   - It relies on preprocessed code data stored in directories like `web_app_vector_storage_metadata` for efficient retrieval.\n",
            "\n",
            "### Summary:\n",
            "The **Android Client** here is not the actual Android app but an **AI-powered assistant** that helps users navigate and understand the Kotlin codebase of Mifos' legacy Android application. It serves as a knowledge base for developers working on or maintaining that project.\n"
          ]
        }
      ]
    }
  ]
}